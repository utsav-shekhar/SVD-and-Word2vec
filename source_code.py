# -*- coding: utf-8 -*-
"""source_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cu82R_rBP_JZikjPQTnVPqXbglqOdl2D
"""

import json
import pandas as pd
import regex as re
import string
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
import random
from tqdm import tqdm
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import gensim.downloader as api
import pickle

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

data=[]
count=40000
i=0
with open('/content/gdrive/MyDrive/reviews_Movies_and_TV.json') as f:
  for line in f:
    i=i+1
    data.append(json.loads(line))
    if(i==count):
      break

df = pd.DataFrame(data)
reviews = (df['reviewText'])

review = list(reviews)

corpus=[]
for entry in review:
  all=sent_tokenize(entry)
  for i in all:
    corpus.append(word_tokenize(i))

  l=len(corpus)
  if l >= 40000:
    break

word_list=[]
for sentence in corpus:
  for word in sentence:
    if word.lower().isalpha()==True:
      word_list.append(word.lower())
print(len(word_list))

print(word_list[1:100])

def word_frequency(sentences):
    word_freq = {}
    for sentence in sentences:
        for word in sentence:
            word=word.lower()
            if not word.isnumeric():
                if word not in word_freq:
                    word_freq[word] = 1
                word_freq[word] += 1
    return word_freq

freq_dict = word_frequency(corpus)

len(freq_dict)

freq_dict['titanic']

unk_freq = sum([value for value in freq_dict.values() if value < 3])
new_dict = {}
replace = []
for key, value in freq_dict.items():
    if value >= 4:
        new_dict[key] = value
    else:
        replace.append(key)
        unk_freq += value

new_dict['UNK'] = unk_freq

print(new_dict)

print(replace)

print(len(new_dict))
print(len(replace))
print(len(freq_dict))

len(new_dict)

new_dict['titanic']

for j in range(len(word_list)):
  if word_list[j] in replace:
        word_list[j]='UNK'

word_index={}
for i in new_dict.keys():
    word_index[i]=len(word_index)

print(word_index)

word_index['titanic']

"""**Cooccurence matrix and single value decomposition model**"""

vocabulary=len(word_index)
coocc_matrix = np.zeros((vocabulary, vocabulary), dtype=np.int32)
window_size=2
for i in range(len(word_list)):
    for j in range(max(0, i - window_size), min(len(word_list), i + window_size + 1)):
        if i != j:
            coocc_matrix[word_index[word_list[i]], word_index[word_list[j]]] += 1

print(coocc_matrix)

np.savetxt('co_occurrence_matrix.txt', coocc_matrix, delimiter=' ')

coocc_matrix_loaded = np.loadtxt('co_occurrence_matrix.txt')

with open('co_occurrence_matrix.pkl', 'wb') as f:
    pickle.dump(coocc_matrix_loaded, f)

U, s, Vh = np.linalg.svd(coocc_matrix)
k = 300
U_k = U[:, :k]
s_k = s[:k]
Vh_k = Vh[:k, :]

word_embeddings = U_k @ np.diag(np.sqrt(s_k))

print(word_embeddings)

targeted = ['helping','collection', 'the', 'class', 'great']
for i in range(0,len(targeted)):

    co_occurrence_matrix = coocc_matrix

    idx2word = {i: word for word, i in word_index.items()}


    target_word = targeted[i]

    target_word_index = word_index[target_word]

    cosine_similarities = np.dot(word_embeddings, word_embeddings[target_word_index]) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_embeddings[target_word_index]))

    closest_words = np.argsort(cosine_similarities)[::-1][:10]

    tsne = TSNE(n_components=2, random_state=0)
    word_embeddings_2d = tsne.fit_transform(word_embeddings)

    for word_idx in closest_words:
        if word_idx not in idx2word:
            continue
        word = idx2word[word_idx]
        x, y = word_embeddings_2d[word_idx]
        plt.scatter(x, y)
        plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')
    plt.show()

titanic_index = word_index['titanic']

cosine_similarities = np.dot(word_embeddings, word_embeddings[titanic_index]) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_embeddings[titanic_index]))

closest_word_indices = np.argsort(cosine_similarities)[::-1][1:11] 

closest_words = [idx2word[idx] for idx in closest_word_indices]
print("Top 10 closest words to 'titanic':")
print(closest_words)

print(word_index.keys())

len(word_index)

"""**CBOW MODEL**"""

torch.manual_seed(1)
context_size = 2
embedding_dim = 300
#represents the no of samples per normal word
num_neg_samples = 5 

#the prev context vector on vsc
def make_context_vector(context, word_index):
    idxs = [word_index[w] for w in context]
    return torch.tensor(idxs, dtype=torch.long).cuda()

vocab = set(word_list)
vocab_size = len(vocab)

word_index = {word: i for i, word in enumerate(vocab)}
idx_to_word = {i: word for i, word in enumerate(vocab)}

data = []

for i in range(2, len(word_list) - 2):
    context = [word_list[i-2], word_list[i-1],
               word_list[i+1], word_list[i+2]]
    target = word_list[i]
    data.append((context, target))

word_freq = {}
for word in word_list:
    if word in word_freq:
        word_freq[word] += 1
    else:
        word_freq[word] = 1


#Main model class

class CBOW(nn.Module):
    
    def __init__(self, vocab_size, embedding_dim):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.proj = nn.Linear(embedding_dim, 128)
        self.output = nn.Linear(128, vocab_size)
        
    def forward(self, inputs):
        embeds = sum(self.embeddings(inputs)).view(1, -1)
        out = F.relu(self.proj(embeds))
        out = self.output(out)
        nll_prob = F.log_softmax(out, dim=-1)
        return nll_prob

#gpu usage
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CBOW(vocab_size, embedding_dim).to(device)
optimizer = optim.SGD(model.parameters(), lr=0.001)

losses = []
loss_function = nn.NLLLoss().to(device)

for epoch in range(10):
    total_loss = 0
    for context, target in tqdm(data):
        pos_samples = [target]
        neg_samples = []
        while len(neg_samples) < num_neg_samples:
            word = random.sample(vocab, 1)[0]
            if word != target and word not in context:
                neg_samples.append(word)
        
        context_vector = make_context_vector(context, word_index).to(device)
        
        model.zero_grad()
        
        nll_prob_pos = model(context_vector)
        loss_pos = loss_function(nll_prob_pos, Variable(torch.tensor([word_index[target]])).to(device))
        
        loss_neg = 0
        for neg_sample in neg_samples:
            nll_prob_neg = model(context_vector)
            loss_neg += loss_function(nll_prob_neg, Variable(torch.tensor([word_index[neg_sample]])).to(device))
        
        loss = loss_pos + loss_neg
       
        loss.backward()
        optimizer.step()

PATH = "/content/pretrained_model.pt"
torch.save(model.state_dict(),PATH)

torch.save(model.state_dict(), 'my_model.pth')

word2vec_embeddings = model.embeddings.weight.detach().cpu().numpy()

with open('word2vec_embeddings.pkl', 'wb') as f:
    pickle.dump(word_index, f)

targeted = ['helping','collection', 'the', 'class', 'great']
for i in range(0,len(targeted)):

    co_occurrence_matrix = coocc_matrix

    idx2word = {i: word for word, i in word_index.items()}


    target_word = targeted[i]

    target_word_index = word_index[target_word]

    cosine_similarities = np.dot(word_embeddings, word_embeddings[target_word_index]) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_embeddings[target_word_index]))

    closest_words = np.argsort(cosine_similarities)[::-1][:10]

    tsne = TSNE(n_components=2, random_state=0)
    word_embeddings_2d = tsne.fit_transform(word_embeddings)

    for word_idx in closest_words:
        if word_idx not in idx2word:
            continue
        word = idx2word[word_idx]
        x, y = word_embeddings_2d[word_idx]
        plt.scatter(x, y)
        plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')
    plt.show()

titanic_index = word_index['titanic']

cosine_similarities = np.dot(word_embeddings, word_embeddings[titanic_index]) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_embeddings[titanic_index]))

closest_word_indices = np.argsort(cosine_similarities)[::-1][1:11] 

closest_words = [idx2word[idx] for idx in closest_word_indices]
print("Top 10 closest words to 'titanic':")
print(closest_words)

model = api.load('word2vec-google-news-300')

word_embedding = model['titanic']

similar_words = model.most_similar('titanic')

similar_words